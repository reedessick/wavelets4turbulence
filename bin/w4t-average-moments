#!/usr/bin/env python3

"""a simple script to pick up moments from multiple snapshots and average them assuming their all drawn from a stationary process
"""
__author__ = "Reed Essick (reed.essick@gmail.com)"

#-------------------------------------------------

import os

import numpy as np
import h5py

import matplotlib
matplotlib.use("Agg")
from matplotlib import pyplot as plt
plt.rcParams['text.usetex'] = True

from argparse import ArgumentParser

#-------------------------------------------------

TICK_PARAMS = dict(
    left=True,
    right=True,
    top=True,
    bottom=True,
    direction='in',
    which='both',
)

SUBPLOTS_ADJUST = dict(
    left=0.12,
    right=0.95,
    bottom=0.10,
    top=0.92,
    hspace=0.03,
)

#-------------------------------------------------

parser = ArgumentParser()

#---

parser.add_argument('inpaths', type=str, nargs='+',
    help='path to an HDF files containing moments')

parser.add_argument('--outpath', type=str, required=True,
    help='the path into which the combined moments will be written')

#---

parser.add_argument('-v', '--verbose', default=False, action='store_true')
parser.add_argument('-V', '--Verbose', default=False, action='store_true')

#---

args = parser.parse_args()

num_inpaths = len(args.inpaths)
assert num_inpaths > 1, 'must supply at least 2 inpaths'

os.makedirs(os.path.abspath(os.path.dirname(args.outpath)), exist_ok=True)

args.verbose |= args.Verbose

#-------------------------------------------------

# load data from first file

if args.verbose:
    print('loading moments from: '+args.inpaths[0])

data = dict()
with h5py.File(args.inpaths[0], 'r') as obj:
    for field in obj.keys():
        if args.Verbose:
            print('    loading: '+field)
        data[field] = dict((key, obj[field][key][:]) for key in obj[field].keys())

#------------------------

# load data from the rest of the files, requiring them to match the format of the first one

# compute the (co)variance of the point estiamtes
for field in data.keys():
    num_scales, num_moments = data[field]['moments'].shape
    data[field]['point_estimate_covariance'] = np.zeros((num_scales, num_moments, num_moments), dtype=float)
    for snd in range(num_scales):
        data[field]['point_estimate_covariance'][snd] += np.outer(data[field]['moments'][snd], data[field]['moments'][snd])

# iterate over the rest of the paths
for path in args.inpaths[1:]:
    if args.verbose:
        print('loading moments from: '+path)

    with h5py.File(path, 'r') as obj:

        # basic sanity checking
        fields = list(obj.keys())
        assert len(fields) == len(data.keys()), 'mismatch in keys'
        assert all(field in data for field in fields), 'mismatch in keys'

        # iterate over fields, checking format and adding data
        for field in fields:
            if args.Verbose:
                print('    loading: '+field)

            # check for consistent structure within the data
            assert np.all(obj[field]['scales'][:] == data[field]['scales'])
            assert np.all(obj[field]['index'][:] == data[field]['index'])

            # add data
            data[field]['moments'] += obj[field]['moments'][:]
            data[field]['covariance'] += obj[field]['covariance'][:]
            for snd in range(len(data[field]['moments'])):
                data[field]['point_estimate_covariance'][snd] += np.outer(obj[field]['moments'][snd,:], obj[field]['moments'][snd,:])

#------------------------

# normalize the averages
if args.verbose:
    print('normalizing')

for field in data.keys():
    # these are simple averages over the inpaths
    data[field]['moments'] = data[field]['moments'] / num_inpaths
    for snd in range(len(data[field]['moments'])):
        data[field]['point_estimate_covariance'][snd] = data[field]['point_estimate_covariance'][snd] / num_inpaths \
            - np.outer(data[field]['moments'][snd], data[field]['moments'][snd])

    # add in the sum of individual covariances
    data[field]['average_covariance'] = data[field]['covariance'] / num_inpaths
    data[field]['covariance'] = (data[field]['point_estimate_covariance'] + data[field]['average_covariance']) / num_inpaths

#------------------------

# write the combined data to disk
if args.verbose:
    print('writing averaged moments to: '+args.outpath)

with h5py.File(args.outpath, 'w') as obj:
    for field in data.keys():
        # write data into HDF
        grp = obj.create_group(field)

        grp.create_dataset('scales', data=data[field]['scales'])
        grp.create_dataset('index', data=data[field]['index'])

        grp.create_dataset('moments', data=data[field]['moments'])
        grp.create_dataset('covariance', data=data[field]['covariance'])

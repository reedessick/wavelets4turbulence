#!/usr/bin/env python3

"""a simple script to load data and compute moments
"""
__author__ = "Reed Essick (reed.essick@gmail.com)"

#-------------------------------------------------

import os

import numpy as np
import h5py

from argparse import ArgumentParser

### non-standard libraries
from w4t import haar, utils

#-------------------------------------------------

DEFAULT_MOMENTS = [2, 3, 4, 5, 6]

#-------------------------------------------------

parser = ArgumentParser()

#---

parser.add_argument('inpath', type=str,
    help='path to an HDF file containing simulation data')

parser.add_argument('outpath', type=str,
    help='path into which moments will be written')

#---

parser.add_argument('--denoise', default=False, action='store_true',
    help='de-noise the wavelet decomposition before plotting')

#---

parser.add_argument('-m', '--moments', default=DEFAULT_MOMENTS, type=str, nargs='+',
    help='the moments to be computed. DEFAULT="%s"' % (', '.join('%d'%_ for _ in DEFAULT_MOMENTS)))

parser.add_argument('-f', '--field', required=True, default=[], type=str, action='append',
    help='load and manipulate this field. Can be repeated. eg, --field vel --field mag --field dens')

parser.add_argument('--polyfit', default=None, type=int,
    help='the order polynomial to fit in the log-log plane of detail coeff vs scale')

parser.add_argument('--polyfit-scales', nargs=2, type=int, default=[], action='append',
    help='only consider fits between these scales. Can be repeated to examing multiple fits (e.g., broken power laws)')

parser.add_argument('--max-edgelength', default=None, type=int,
    help='if specified, limit the size of each dimension to be at most this many samples. Although not required, \
it is a good idea to make this a power of 2 (for the Haar transform to work well)')

#---

parser.add_argument('-v', '--verbose', default=False, action='store_true')
parser.add_argument('-V', '--Verbose', default=False, action='store_true')

#---

args = parser.parse_args()

os.makedirs(os.path.dirname(os.path.abspath(args.outpath)), exist_ok=True)

args.verbose |= args.Verbose

#-------------------------------------------------

if args.verbose:
    print('loading data from: '+args.inpath)
    print('writing moments into: '+args.outpath)

with h5py.File(args.outpath, 'w') as obj:

    for field in args.field:

        if args.verbose:
            print('examining field: '+field)

        data = utils.load(
            [field],
            path=args.inpath,
            max_edgelength=args.max_edgelength,
            verbose=args.verbose,
            Verbose=args.Verbose,
        )[field]

        if (len(data) > 1): # this is a vector, so compute a few different versions of it
            data = [
                (field+'_x', data[0]),
                (field+'_y', data[1]),
                (field+'_z', data[2]),
                (field+'_mag', np.sum(data**2, axis=0)**0.5)
            ]
        else:
            data = [(field, data[0])]

        for label, datum in data:

            if args.Verbose:
                print('    examining '+label)

            # basic instantiation
            ha = haar.HaarArray(datum)

            if args.denoise:
                if args.Verbose:
                    print('    denoising data')
                thr = ha.denoise() # denoise
                ha.idecompose() # got back to the starting point
                if args.Verbose:
                    print('    retaining only detail coeffs larger than thr=%.6f' % thr)

            #------------

            # compute spectra
            if args.Verbose:
                print('    computing moments')

            scales, mom, cov = ha.spectrum(index=args.moments, use_abs=True)

            #------------
            # fit moments with a power-law

            if args.polyfit is not None:
                if args.Verbose:
                    print('    fitting moments vs scale')

                poly = [[] for _ in args.moments]
                polybins = [[] for _ in args.moments]

                for ind, index in enumerate(args.moments):
                    sel = cov[:,ind,ind] > 0 # restrict ourselves to indexes with reasonable values

                    # iterate over bins for min, max scales
                    if not args.polyfit_scales:
                        bins = [(np.min(scales[sel,0]), np.max(scales[sel,0]))] # by default, use the full range
                    else:
                        bins = args.polyfit_scales

                    for m, M in bins:
                        ect = sel * (m <= scales[:,0]) * (scales[:,0] <= M)

                        err = 1/mom[ect,ind] * cov[ect,ind,ind]**0.5

                        p = np.polyfit(
                            np.log(scales[ect,0]),
                            np.log(mom[ect,ind]),
                            args.polyfit, # fit this degree polynomial
                            w=1./err, # weigh by inverse-variance
                        )

                        poly[ind].append(p)
                        polybins[ind].append((m,M))

            #------------

            # write data into HDF
            grp = obj.create_group(label)

            grp.create_dataset('scales', data=scales)
            grp.create_dataset('index', data=args.moments)

            grp.create_dataset('moments', data=mom)
            grp.create_dataset('covariance', data=cov)

            if args.polyfit is not None:
                grp.create_dataset('poly', data=poly)
                grp.create_dataset('polybins', data=polybins)

            #----------------

            del ha

        del data

#!/usr/bin/env python3

"""a simple script to load data, decompose it, and write the decomposed data to disk
"""
__author__ = "Reed Essick (reed.essick@gmail.com)"

#-------------------------------------------------

import os

import numpy as np
import h5py

from argparse import ArgumentParser

### non-standard libraries
from w4t import haar, utils

#-------------------------------------------------

parser = ArgumentParser()

#---

parser.add_argument('inpath', type=str,
    help='path to an HDF file containing simulation data')

parser.add_argument('outpath', type=str,
    help='path into which moments will be written')

#---

parser.add_argument('--denoise', default=None, type=float)

#---

parser.add_argument('-f', '--field', required=True, default=[], type=str, action='append',
    help='load and manipulate this field. Can be repeated. eg, --field vel --field mag --field dens')

parser.add_argument('-m', '--max-edgelength', default=None, type=int,
    help='if specified, limit the size of each dimension to be at most this many samples. Although not required, \
it is a good idea to make this a power of 2 (for the Haar transform to work well)')

#---

parser.add_argument('-v', '--verbose', default=False, action='store_true')
parser.add_argument('-V', '--Verbose', default=False, action='store_true')

#---

args = parser.parse_args()

os.makedirs(os.path.dirname(os.path.abspath(args.outpath)), exist_ok=True)

args.verbose |= args.Verbose

#-------------------------------------------------

if args.verbose:
    print('loading data from: '+args.inpath)
    print('writing decomposed data into: '+args.outpath)

with h5py.File(args.outpath, 'w') as obj:

    for field in args.field:

        if args.verbose:
            print('examining field: '+field)

        data = utils.load(
            [field],
            path=args.inpath,
            max_edgelength=args.max_edgelength,
            verbose=args.verbose,
            Verbose=args.Verbose,
        )[field]

        if (len(data) > 1): # this is a vector, so compute a few different versions of it
            data = [
                (field+'_x', data[0]),
                (field+'_y', data[1]),
                (field+'_z', data[2]),
                (field+'_mag', np.sum(data**2, axis=0)**0.5)
            ]
        else:
            data = [(field, data[0])]

        for label, datum in data:

            if args.Verbose:
                print('    examining '+label)

            # basic instantiation
            ha = haar.HaarArray(datum)

            if args.denoise is not None:
                if args.Verbose:
                    print('    denoising with thr=%.3f'%args.denoise)
                ha.denoise(args.denoise)

            #--------------------

            # compute spectra
            if args.Verbose:
                print('    decomposing')

            ha.decompose()

            #----------------

            # write data into HDF
            grp = obj.create_group(label)

            grp.create_dataset('array', data=ha._array)
            grp.create_dataset('shape', data=ha._shape)
            grp.create_dataset('ndim', data=ha._ndim)
            grp.create_dataset('levels', data=ha._levels)

            #----------------

            del ha

        del data
